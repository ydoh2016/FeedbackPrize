{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feeddback_Prize.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ydoh2016/FeedbackPrize/blob/master/Feeddback_Prize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FHiQm0p_eS6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXMMzEvhNB6z"
      },
      "outputs": [],
      "source": [
        "# Then move kaggle.json into the folder where the API expects to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv drive/MyDrive/kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle competitions download -c feedback-prize-2021"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip train.csv.zip"
      ],
      "metadata": {
        "id": "QZvepehPcHpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "kwIVA3rxnTg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"train.csv\")\n",
        "print(data.columns)\n",
        "print(data[\"discourse_type\"])"
      ],
      "metadata": {
        "id": "a1IJC2XOcS9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"train.csv\", usecols = ['discourse_text','discourse_type'])\n",
        "data['discourse_type'] = data['discourse_type'].replace(['Lead', 'Position', 'Evidence', 'Counterclaim', 'Claim', 'Concluding Statement', 'Rebuttal'],[0,1,2,3,4,5,6])\n",
        "print(data.head())\n",
        "data.to_pickle(\"train.pkl\")"
      ],
      "metadata": {
        "id": "PoQWrIGBoCLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import timedelta\n",
        "import time\n",
        "from random import random\n",
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "from transformers import BertTokenizerFast, AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))  # 반올림\n",
        "    return str(timedelta(seconds=elapsed_rounded))  # hh:mm:ss으로 형태 변경\n",
        "\n",
        "\n",
        "class Classification:\n",
        "    def __init__(self, model_name='kykim/bert-base-uncased', model_dir='model', min_sentence_length=1,\n",
        "                 MAX_LEN=256, batch_size=64, use_bert_tokenizer=True):\n",
        "\n",
        "        # Init variable.\n",
        "        self.model_name = model_name\n",
        "        self.MAX_LEN = MAX_LEN  # 입력 토큰의 최대 시퀀스 길이\n",
        "        self.batch_size = batch_size  # 배치 사이즈\n",
        "        self.min_sentence_length = min_sentence_length\n",
        "\n",
        "        # path\n",
        "        self.model_dir = model_dir\n",
        "        self.save_path = os.path.join(model_dir, 'saved')\n",
        "        if not os.path.exists(self.save_path):\n",
        "            os.makedirs(self.save_path)\n",
        "\n",
        "        # 디바이스 설정\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = torch.device(\"cuda\")\n",
        "            # print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "            # print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "        else:\n",
        "            self.device = torch.device(\"cpu\")\n",
        "            print('No GPU available, using the CPU instead.')\n",
        "\n",
        "        # MODEL\n",
        "        self.tokenizer_class = AutoTokenizer\n",
        "        self.model_class = AutoModelForSequenceClassification\n",
        "\n",
        "        if use_bert_tokenizer:\n",
        "            self.tokenizer_class = BertTokenizerFast\n",
        "\n",
        "    def dataset(self, data_path):\n",
        "        data = pd.read_pickle(data_path)\n",
        "\n",
        "        self.sentences, self.labels = [], []\n",
        "        for idx in data.index:\n",
        "            sentence = data.loc[idx, 'discourse_text']\n",
        "            label = data.loc[idx, 'discourse_type']\n",
        "            print(label)\n",
        "            if len([i for i in str(sentence) if i != ' ']) < self.min_sentence_length:\n",
        "                continue\n",
        "            if not pd.isnull(label):\n",
        "                self.labels.append(int(label))\n",
        "                self.sentences.append(str(sentence))\n",
        "            else:\n",
        "                print('label error:', sentence, label)\n",
        "\n",
        "        self. num_labels = len(list(set(self.labels)))\n",
        "        print('{} labels, {} dataset'.format(self.num_labels, len(self.labels)))\n",
        "        print('label counts:: {}'.format(Counter(self.labels)))\n",
        "\n",
        "    def load_model(self, mode=None, saved_model_path=None):\n",
        "        self.tokenizer = self.tokenizer_class.from_pretrained(self.model_name)\n",
        "\n",
        "        if mode == 'train':\n",
        "            self.model = self.model_class.from_pretrained(self.model_name, num_labels=self.num_labels)\n",
        "        elif mode == 'inference':\n",
        "            self.model = self.model_class.from_pretrained(saved_model_path)\n",
        "\n",
        "        if self.device == torch.device(\"cuda\"):\n",
        "            self.model.cuda()\n",
        "\n",
        "    def tokenizing(self, mode='train', dataset_split=0.1):\n",
        "        if mode == 'train':\n",
        "            input_ids = [self.tokenizer.encode(sentence, padding='max_length',\n",
        "                                               max_length=self.MAX_LEN,\n",
        "                                               truncation=True,\n",
        "                                               ) for sentence in self.sentences]\n",
        "\n",
        "            # 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n",
        "            attention_masks = []\n",
        "            for seq in input_ids:\n",
        "                seq_mask = [float(i > 0) for i in seq]\n",
        "                attention_masks.append(seq_mask)\n",
        "\n",
        "            # 훈련셋과 검증셋으로 분리\n",
        "            train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, self.labels,\n",
        "                                                                                                random_state=2021,\n",
        "                                                                                                test_size=dataset_split)\n",
        "\n",
        "            train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids, random_state=2021,\n",
        "                                                                   test_size=dataset_split)\n",
        "\n",
        "            # 데이터를 파이토치의 텐서로 변환\n",
        "            train_inputs = torch.tensor(train_inputs)\n",
        "            train_labels = torch.tensor(train_labels)\n",
        "            train_masks = torch.tensor(train_masks)\n",
        "            validation_inputs = torch.tensor(validation_inputs)\n",
        "            validation_labels = torch.tensor(validation_labels)\n",
        "            validation_masks = torch.tensor(validation_masks)\n",
        "\n",
        "            train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "            train_sampler = RandomSampler(train_data)\n",
        "            self.train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=self.batch_size)\n",
        "            validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "            validation_sampler = SequentialSampler(validation_data)\n",
        "            self.validation_dataloader = DataLoader(validation_data, sampler=validation_sampler,\n",
        "                                                    batch_size=self.batch_size)\n",
        "            print('{}-dataset is prepared'.format(mode))\n",
        "\n",
        "        elif mode == 'inference':\n",
        "            input_ids = [self.tokenizer.encode(sentence, padding='max_length',\n",
        "                                               max_length=self.MAX_LEN,\n",
        "                                               truncation=True,\n",
        "                                               ) for sentence in self.sentences]\n",
        "\n",
        "            # 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n",
        "            attention_masks = []\n",
        "            for seq in input_ids:\n",
        "                seq_mask = [float(i > 0) for i in seq]\n",
        "                attention_masks.append(seq_mask)\n",
        "\n",
        "            inputs = torch.tensor(input_ids).to(self.device)\n",
        "            masks = torch.tensor(attention_masks).to(self.device)\n",
        "\n",
        "            return inputs, masks\n",
        "\n",
        "    def inference(self, sentences):\n",
        "        self.sentences = sentences\n",
        "\n",
        "        model = self.model\n",
        "        model.eval()\n",
        "\n",
        "        # tokenizer\n",
        "        b_input_ids, b_input_mask = self.tokenizing(mode='inference')\n",
        "\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "        except Exception as E:\n",
        "            print(E)\n",
        "            exit()\n",
        "        logits = outputs[0]\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "\n",
        "        return [np.argmax(logit) for logit in logits]\n",
        "\n",
        "\n",
        "    def train(self, epochs=1, log_dir='log', dataset_split=0.1):\n",
        "        # tokenizer\n",
        "        self.tokenizing(mode='train', dataset_split=dataset_split)\n",
        "\n",
        "        writer = SummaryWriter(log_dir)\n",
        "\n",
        "        model = self.model\n",
        "        optimizer = AdamW(model.parameters(), lr=3e-5, eps=1e-8)\n",
        "\n",
        "        # 총 훈련 스텝 : 배치반복 횟수 * 에폭\n",
        "        total_steps = len(self.train_dataloader) * epochs\n",
        "\n",
        "        # 학습률을 조금씩 감소시키는 스케줄러 생성\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "        # 재현을 위해 랜덤시드 고정\n",
        "        seed_val = 42\n",
        "        random.seed(seed_val)\n",
        "        np.random.seed(seed_val)\n",
        "        torch.manual_seed(seed_val)\n",
        "        torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "        # 그래디언트 초기화\n",
        "        model.zero_grad()\n",
        "\n",
        "        # 에폭만큼 반복\n",
        "        for epoch_i in range(0, epochs):\n",
        "            print(\"\")\n",
        "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "            print('Training...')\n",
        "            t0 = time.time()\n",
        "            total_loss = 0\n",
        "            train_accuracy, nb_train_steps = 0, 0\n",
        "\n",
        "            model.train()\n",
        "\n",
        "            # 데이터로더에서 배치만큼 반복하여 가져옴\n",
        "            for step, batch in enumerate(self.train_dataloader):\n",
        "                # 경과 정보 표시\n",
        "                if step % 100 == 0 and not step == 0:\n",
        "                    elapsed = format_time(time.time() - t0)\n",
        "                    print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(self.train_dataloader),\n",
        "                                                                                elapsed))\n",
        "\n",
        "                batch = tuple(t.to(self.device) for t in batch)  # 배치를 GPU에 넣음\n",
        "                b_input_ids, b_input_mask, b_labels = batch  # 배치에서 데이터 추출\n",
        "                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask,\n",
        "                                labels=b_labels)  # Forward 수행\n",
        "                loss = outputs[0]  # 로스 구함\n",
        "                total_loss += loss.item()  # 총 로스 계산\n",
        "\n",
        "                loss.backward()  # Backward 수행으로 그래디언트 계산\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # 그래디언트 클리핑\n",
        "                optimizer.step()  # 그래디언트를 통해 가중치 파라미터 업데이트\n",
        "\n",
        "                scheduler.step()  # 스케줄러로 학습률 감소\n",
        "                model.zero_grad()  # 그래디언트 초기화\n",
        "\n",
        "                ##accuracy\n",
        "                logits = outputs[1]\n",
        "                # CPU로 데이터 이동\n",
        "                logits = logits.detach().cpu().numpy()\n",
        "                label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "                # 출력 로짓과 라벨을 비교하여 정확도 계산\n",
        "                train_accuracy += flat_accuracy(logits, label_ids)\n",
        "                nb_train_steps += 1\n",
        "\n",
        "            # 평균 로스 계산\n",
        "            avg_train_loss = total_loss / len(self.train_dataloader)\n",
        "\n",
        "            print(\"\")\n",
        "            print(\"  Train loss: {0:.2f}, Train Accuracy: {1:.2f}\".format(avg_train_loss,\n",
        "                                                                          train_accuracy / nb_train_steps))\n",
        "            print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "            # ========================================\n",
        "            #               Validation\n",
        "            # ========================================\n",
        "\n",
        "            print(\"\")\n",
        "            print(\"Running Validation...\")\n",
        "\n",
        "            t0 = time.time()\n",
        "            model.eval()\n",
        "\n",
        "            # 변수 초기화\n",
        "            eval_loss, eval_accuracy = 0, 0\n",
        "            nb_eval_steps = 0\n",
        "            labels_accuracy, preds_accuracy = [], []\n",
        "\n",
        "            # 데이터로더에서 배치만큼 반복하여 가져옴\n",
        "            for batch in self.validation_dataloader:\n",
        "                batch = tuple(t.to(self.device) for t in batch)\n",
        "                b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "\n",
        "                logits = outputs[0]\n",
        "                # CPU로 데이터 이동\n",
        "                logits = logits.detach().cpu().numpy()\n",
        "                label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "                # 출력 로짓과 라벨을 비교하여 정확도 계산\n",
        "                tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "                eval_accuracy += tmp_eval_accuracy\n",
        "                nb_eval_steps += 1\n",
        "\n",
        "                labels_accuracy.append(label_ids.flatten())\n",
        "                preds_accuracy.append(np.argmax(logits, axis=1).flatten())\n",
        "\n",
        "            print(\"  Validation Accuracy: {0:.2f}\".format(eval_accuracy / nb_eval_steps))\n",
        "            print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "            # precision and recall\n",
        "            labels_accuracy = [y for x in labels_accuracy for y in x]  # list flatten\n",
        "            preds_accuracy = [y for x in preds_accuracy for y in x]\n",
        "            print(classification_report(labels_accuracy, preds_accuracy))\n",
        "\n",
        "            writer.add_scalar('Avg_loss(training)', avg_train_loss, epoch_i + 1)\n",
        "            writer.add_scalars('Accuracy', {'Train': train_accuracy / nb_train_steps,\n",
        "                                            'Val': eval_accuracy / nb_eval_steps}, epoch_i + 1)\n",
        "\n",
        "            if (epoch_i + 1) % 3 == 0 and (epoch_i + 1) != epochs:  ##마지막 iteration은 아래에서 수행.\n",
        "                save_path = os.path.join(self.save_path, str(epoch_i + 1))\n",
        "                if not os.path.exists(save_path): os.makedirs(save_path)\n",
        "\n",
        "                model.save_pretrained(save_path)\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"Training complete!\")\n",
        "        writer.close()\n",
        "\n",
        "        save_path = os.path.join(self.save_path, str(epochs), '_5class_stmixed')\n",
        "        if not os.path.exists(save_path): os.makedirs(save_path)\n",
        "\n",
        "        model.save_pretrained(save_path)"
      ],
      "metadata": {
        "id": "w2DNxwakmbf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CLS = Classification(model_name=\"bert-base-uncased\", model_dir=\"model\", MAX_LEN=512, batch_size=32)\n",
        "CLS.dataset(\"train.pkl\")\n",
        "CLS.load_model(mode=\"train\")\n",
        "CLS.train(epochs=5, log_dir=\"log\", dataset_split=0.01)"
      ],
      "metadata": {
        "id": "5DA35NqblRR8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}